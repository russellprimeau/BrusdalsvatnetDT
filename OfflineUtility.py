import datetime
import numpy as np
import pandas as pd
import streamlit as st
import os
import tempfile2 as tempfile
import re
import xarray as xr
import xugrid as xu
import matplotlib.pyplot as plt
import contextily as ctx
import dfm_tools as dfmt
from sklearn.linear_model import LinearRegression
from scipy.optimize import minimize
import plotly.express as px
import Dashboard


def est_his_size(start=None, end=None, h_step=None, h1=1, h2=5, h3=2, h4=None):
    """
    Estimate the size of the history file output which will be generated by the file configuration
    :param start: simulation start time
    :param end: simulation end time
    :param h_step: output interval
    :param h1: # of observation points, cross-sections, etc. written
    :param h2: # of quantities stored by default
    :param h3: # of process parameters (salinity, temperature, constituents, turbulence quantities)
    :param h4: (optional) # of time steps
    :return: file size, kB
    """
    if h4 is None:
        h4 = (end - start) / h_step
    if start is None and end is None and h_step is None and h4 is None:
        h4 = 1
        print("Estimated 'his' size is per output interval")
    return (h1 * (h2 + h3) * h4 * 8)/1024


def est_map_size(start=None, end=None, m_step=None, Ndxi=1, Kmax=1, Lnx=1, m1n=None, m1l=None, m2n=5, m2l=5, m3=2,
                 m4=None):
    """
    Estimate the size of the map file output which will be generated by the file configuration
    *Note the formula is based on sigma layers, so will tend to overestimate file size for z-layer models
    :param start: simulation start time
    :param end: simulation end time
    :param m_step: output interval
    :param Ndxi: # of grid cells
    :param Kmax: # of layers
    :param m1n: (optional) total # of 3d cells
    :param Lnx: # of flow links
    :param m1l: (optional) total # of flow links
    :param m2n: # of quantities stored in grid cells
    :param m2l: # of quantities stored in flow links
    :param m3: # of process parameters (salinity, temperature, constituents, turbulence quantities)
    :param m4: (optional) # of time steps written
    :return: file size, kB
    """
    if m1n is None:
        m1n = Ndxi * Kmax
    if m1l is None:
        m1l = Lnx * Kmax
    if m4 is None:
        m4 = (end - start) / m_step
    if start is None and end is None and m_step is None and m4 is None:
        m4 = 1
        print("Estimated 'map' size is per output interval")
    return (((m1n * (m2n + m3)) + (m1l * m2l)) * m4 * 8)/1024


def gen_MDU(all_files):
    """
    Make adjustments to the MDU file, which defines all simulation settings for Delft3D FM.
    Running a simulation programatically entails calling a batch file, which calls the Delft 3D executable on a
    DIMR configuration file. The DIMR configuration file defines paths to the input directory, where the executable
    finds the .MDU file, as well as an output directory for the executable to save output files (.dia, _map.nc,
    _his.nc, etc.)
    :return:
    """

    # Get all files in the current directory
    st.subheader("Make adjustments to the MDU file")

    filtered_files = [f for f in all_files if f.endswith('.mdu')] + ["Upload your own"]
    hc1, hc2 = st.columns(2, gap="small")
    with hc1:
        selected_file = st.selectbox(label="Select an MDU file to modify", options=filtered_files,
                                     help="The Master Definition Unstructured file (mdu-file) is the input file for the"
                                          " hydrodynamic simulation program. It contains the data required for defining"
                                          " a model and running a simulation. The .mdu file also defines the names of "
                                          "the attribute files in which external input data is stored, such as "
                                          "time-series data for forcing functions.")

    if selected_file == "Upload your own":
        hc1, hc2 = st.columns(2, gap="small")
        with hc1:
            uploaded = st.file_uploader(label='Upload an MDU file', type='mdu')
        # Create a temp filepath to use to access the uploaded file
        if uploaded is not None:
            if uploaded.name.endswith('.mdu'):
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_file.write(uploaded.read())
                    file_path = temp_file.name
            else:
                st.markdown("### File uploaded is not a valid MDU file")

    if uploaded is not None:
        # To read file as string:
        string_data = uploaded.getvalue().decode("utf-8")
        st.text_area("MDU file content:", string_data, height=300)

    if selected_file is not None and uploaded is not None:
        # Define the pattern to search for and the replacement text
        pattern_to_search = st.text_input("Enter the MDU content to replace:")
        replacement_text = st.text_input("Enter the replacement content")

        # Open the file and read its contents
        with open(uploaded, 'r') as file:
            file_contents = file.read()

        # Use regular expressions to find and replace
        modified_contents = re.sub(pattern_to_search, replacement_text, file_contents)

        # Write the modified contents back to the file
        with open('your_file.txt', 'w') as file:
            file.write(modified_contents)

    st.subheader("Estimate output file sizes")
    u1, u2 = st.columns(2, gap='small')
    with u1:
        st.markdown("History (his.nc) file")
        c1, c2 = st.columns(2, gap='small')
        with c1:
            h1 = st.number_input("# Number of observation points", value=1, step=1)
            h3 = st.number_input("# Number of his parameters written", value=2, step=1)
        with c2:
            h4 = st.number_input("# Number of his output time steps written", value=1, step=1)
        his_size = est_his_size(h1=h1, h3=h3, h4=h4)
        st.markdown(f"File size: {his_size} kB")
    with u2:
        st.markdown("Map (map.nc) file")
        c1, c2 = st.columns(2, gap='small')

        with c1:
            Ndxi = st.number_input("# Number of grid cells", value=1, step=1)
            Kmax = st.number_input("# Number of flow links", value=1, step=1)
        with c2:
            m3 = st.number_input("# Number of process parameters written", value=2, step=1)
            m4 = st.number_input("# Number of map output time steps written", value=1, step=1)
        map_size = est_map_size(Ndxi=Ndxi, Kmax=Kmax, m3=m3, m4=m4)
        st.markdown(f"File size: {map_size} kB")


def gen_sens():
    test_options = []
    param = st.selectbox("Select the input parameter for which sensitivity will be tested", test_options)

    # This example attempts to find the values of the input variables (which should be replaced with values of
    # the parameter adjusted in the sensitivity test) which minimize the error term (which will be generated by
    # simulation iterations, presumably depth- and time-averaged.)

    # Assuming df is your DataFrame
    # df = pd.DataFrame({
    #     'input1': [...],
    #     'input2': [...],
    #     'input3': [...],
    #     'input4': [...],
    #     'input5': [...],
    #     'output': [...]
    # })

    df = pd.DataFrame()

    # Separate the input values (features) and the output value (target)
    X = df[['input1', 'input2', 'input3', 'input4', 'input5']]
    y = df['output']

    # Perform multivariate regression
    model = LinearRegression()
    model.fit(X, y)

    # Print out the coefficients and intercept
    print("Coefficients:", model.coef_)
    print("Intercept:", model.intercept_)

    # Define the function to minimize (predicted output of the model)
    def objective_function(inputs):
        return model.predict([inputs])[0]

    # Initial guess for the input values (you can set this to the mean of each input column or any other heuristic)
    initial_guess = X.mean().values

    # Perform optimization to minimize the output value
    result = minimize(objective_function, initial_guess, method='BFGS')

    # Print the results
    if result.success:
        print("Optimization successful!")
        print("Estimated input values to minimize the output value:", result.x)
        print("Minimum output value:", result.fun)
    else:
        print("Optimization failed:", result.message)


def convert_to_minutes_relative(df_convo, datetime_col, ref_time):
    """
    Converts datetime values in a DataFrame column to minutes relative to a reference time.
    :param df_convo: The pandas DataFrame containing the datetime column.
    :param datetime_col: The name of the column containing datetime values.
    :param ref_time: A pandas.Timestamp object representing the reference time.
    :return: A new DataFrame with the same columns as the input DataFrame, with the specified
      column containing the datetime values converted to minutes relative to the reference time.
    """

    # Ensure reference time is a pandas.Timestamp
    if not isinstance(ref_time, pd.Timestamp):
        ref_time = pd.Timestamp(ref_time)

    # Convert datetime column to timedelta (difference from reference time)
    df_convo['time_diff'] = df_convo[datetime_col] - ref_time

    # Convert timedelta to minutes (assuming microseconds are not relevant)
    df_convo['time_diff_minutes'] = df_convo['time_diff'] / pd.Timedelta(minutes=1)

    # Drop the original datetime column if desired
    df_convo = df_convo.drop(datetime_col, axis=1)
    df_convo = df_convo.drop('time_diff', axis=1)
    return df_convo


def filter_profiler(start_date, end_date, reference_time, df, discharge, salinity, scalefactor, randomize_type):
    """
    Creates a dataframe corresponding to the table of time-series input data for a source-sink object in Delft3D FM
    (i.e. a stream flowing into the lake), which can be written to a .tim file. Uses the surface temperatures from
    the vertical profiler station as a proxy for the temperature of the streams.

    :param start_date:
    :param end_date:
    :param reference_time:
    :param df:
    :param discharge:
    :param salinity:
    :param scalefactor: reserved for future use
    :param randomize_type: reserved for future use
    :return: dataframe of: Relative time; Discharge; Salinity; Temperature
    """

    source_columns = ["Temperature (Celsius)"]

    # Drop columns which will not be used in the output
    df = df.drop(columns=["Conductivity (microSiemens/centimeter)",
                          "Specific Conductivity (microSiemens/centimeter)", "Salinity (parts per thousand, ppt)",
                          "pH", "Dissolved Oxygen (% saturation)", "Turbidity (NTU)", "Turbidity (FNU)",
                          "fDOM (RFU)", "fDOM (parts per billion QSU)", "Latitude",
                          "Longitude"])

    # Filter data by date range
    df['Timestamp'] = df['Timestamp'].dt.ceil('H')
    filtered_df = df[(df['Timestamp'] >= start_date) & (df['Timestamp'] <= end_date)]

    # Add columns for data which is not driven by profiler data
    filtered_df["Discharge (m^3/s)"] = discharge
    filtered_df["Salinity (ppt)"] = salinity

    ###################################################################################################################
    # Write wind data file

    # Convert the time column to minutes relative to the reference date
    filtered_df = convert_to_minutes_relative(filtered_df, "Timestamp", reference_time)

    # Example for converting units
    # filtered_df['Hourly precipitation (mm/hr)'] = 24 * filtered_df['Hourly precipitation (mm/hr)']

    # Select columns to include
    col_to_source = ["time_diff_minutes", "Discharge (m^3/s)", "Salinity (ppt)", *source_columns]

    # Format for .tim file (scientific notation, 7 significant digits)
    source_sink_df = filtered_df.assign(**{col: filtered_df[col].apply(lambda x: f'{x:.7e}') for col in col_to_source})
    source_sink_df = source_sink_df[col_to_source]

    return source_sink_df


def filter_met(start_date, end_date, reference_time, df):
    """
    Reads a CSV file, filters data by date range and specified columns,
  performs optional data manipulation, and writes the result to a new CSV file.
    :param start_date: String representing the start date in YYYY-MM-DD format.
    :param end_date: String representing the end date in YYYY-MM-DD format.
    :param reference_time: time relative to which output file time is expressed (in minutes)
    :param df: dataframe into which the csv file with all station data is written
    :return: 3 dataframes with tables of wind, weather and rainfall respectively for the selected time frame
    """
    wind_columns = ["Average wind speed (m/s)", "Hourly average wind direction (°)"]
    met_columns = ["Average humidity (% relative humidity)", "Avg. Temp (°C)", "Cloud cover",
                   "Shortwave (solar) radiation (W/m2)"]
    rain_columns = ["Hourly precipitation (mm/hr)"]

    column_names = {"Time": "Timestamp",
                    "1818_time: AA[mBar]": "Instantaneous atmospheric pressure (mBar)",
                    "1818_time: DD Retning[°]": "Wind direction 10minRollingAvg (°)",
                    "1818_time: DX_l[°]": "Hourly average wind direction (°)",
                    "1818_time: FF Hastighet[m/s]": "Average wind speed (m/s)",
                    "1818_time: FG_l[m/s]": "Maximum sustained wind speed, 3-second span (m/s)",
                    "1818_time: FG_tid_l[N/A]": "Time of maximum 3s Gust",
                    "1818_time: FX Kast[m/s]": "Maximum sustained wind speed, 10-minute span (m/s)",
                    "1818_time: FX_tid_l[N/A]": "Time of maximum 10 minute gust",
                    "1818_time: PO Trykk stasjonshøyde[mBar]": "Hourly average atmospheric pressure at station "
                                                               "(mBar)",
                    "1818_time: PP[mBar]": "Maximum pressure differential, 3-hour span (mBar)",
                    "1818_time: PR Trykk redusert til havnivå[mBar]": "Instantaneous atmospheric pressure "
                                                                      "compensated for temperature, humidity "
                                                                      "and station elevation (mBar)",
                    "1818_time: QLI Langbølget[W/m2]": "Longwave (IR) radiation (W/m2)",
                    "1818_time: QNH[mBar]": "Instantaneous sea-level atmospheric pressure (mBar)",
                    "1818_time: QSI Kortbølget[W/m2]": "Shortwave (solar) radiation (W/m2)",
                    "1818_time: RR_1[mm]": "Hourly precipitation (mm/hr)",
                    "1818_time: TA Middel[°C]": "Instantaneous temperature (°C)",
                    "1818_time: TA_a_Max[°C]": "Hourly maximum temperature (°C)",
                    "1818_time: TA_a_Min[°C]": "Hourly minimum temperature (°C)",
                    "1818_time: UU Luftfuktighet[%RH]": "Average humidity (% relative humidity)"
                    }
    df = df.rename(columns=column_names)  # Assign column names for profiler data

    # Set negative shortwave values to 0 (this is very common and appears to represent a calibration issue)
    df['Shortwave (solar) radiation (W/m2)'] = (
        np.where(df['Shortwave (solar) radiation (W/m2)'] < 0, 0, df['Shortwave (solar) radiation (W/m2)']))

    # Define conditions for each parameter which indicate errors in the data
    error_conditions = {
        "Timestamp": (df['Timestamp'] < pd.to_datetime('2000-01-01')) | (
                df['Timestamp'] > pd.to_datetime('2099-12-31')),
        'Hourly average wind direction (°)': (df['Hourly average wind direction (°)'] < 0) | (
                df['Hourly average wind direction (°)'] > 360),
        "Average wind speed (m/s)": (df["Average wind speed (m/s)"] < 0) | (
                df["Average wind speed (m/s)"] > 100),
        'Maximum sustained wind speed, 3-second span (m/s)': (df[
                                                                  'Maximum sustained wind speed, 3-second span (m/s)'] < 0) |
                                                             (df[
                                                                  'Maximum sustained wind speed, 3-second span (m/s)'] > 100),
        'Maximum sustained wind speed, 10-minute span (m/s)': (
                                                                      df[
                                                                          'Maximum sustained wind speed, 10-minute span (m/s)'] < 0) |
                                                              (df[
                                                                   'Maximum sustained wind speed, 10-minute span (m/s)'] > 100),
        'Hourly average atmospheric pressure at station (mBar)': (df[
                                                                      'Hourly average atmospheric pressure at station (mBar)'] < 860) | (
                                                                         df[
                                                                             'Hourly average atmospheric pressure at station (mBar)'] > 1080),
        'Maximum pressure differential, 3-hour span (mBar)': (df[
                                                                  'Maximum pressure differential, 3-hour span (mBar)'] < 0) | (
                                                                     df[
                                                                         'Maximum pressure differential, 3-hour span (mBar)'] > 50),
        'Longwave (IR) radiation (W/m2)': (df['Longwave (IR) radiation (W/m2)'] < 0) | (
                df['Longwave (IR) radiation (W/m2)'] > 750),
        'Shortwave (solar) radiation (W/m2)': (df['Shortwave (solar) radiation (W/m2)'] < 0) | (
                df['Shortwave (solar) radiation (W/m2)'] > 900),
        'Hourly precipitation (mm/hr)': (df['Hourly precipitation (mm/hr)'] < 0) | (
                df['Hourly precipitation (mm/hr)'] > 50),
        'Hourly maximum temperature (°C)': (df['Hourly maximum temperature (°C)'] < -40) | (
                df['Hourly maximum temperature (°C)'] > 40),
        'Hourly minimum temperature (°C)': (df['Hourly minimum temperature (°C)'] < -40) | (
                df['Hourly minimum temperature (°C)'] > 40),
        'Average humidity (% relative humidity)': (df['Average humidity (% relative humidity)'] < 0) | (
                df['Average humidity (% relative humidity)'] > 100)
    }

    # Replace values meeting the error conditions with np.nan using boolean indexing
    for col, condition in error_conditions.items():
        df.loc[condition, col] = np.nan

    # Data cleaning
    for parameter in df.columns:
        df[parameter] = df[parameter].apply(lambda x: np.nan if x == 'NAN' else x)
    df.iloc[:, 1:] = df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce', downcast='float')

    # Sort by the time column
    df = df.sort_values(by="Timestamp")

    # Drop columns which will not be used in the output
    df = df.drop(columns=["Instantaneous atmospheric pressure (mBar)", "Wind direction 10minRollingAvg (°)",
                          "Time of maximum 3s Gust", "Time of maximum 10 minute gust",
                          "Instantaneous atmospheric pressure compensated for temperature, humidity and station "
                          "elevation (mBar)",
                          "Instantaneous sea-level atmospheric pressure (mBar)", "Instantaneous temperature (°C)"])

    # Filter data by date range
    filtered_df = df[(df['Timestamp'] >= start_date) & (df['Timestamp'] <= end_date)]

    # Calculate "average" temperature:
    filtered_df["Avg. Temp (°C)"] = (df['Hourly maximum temperature (°C)'] + df['Hourly minimum temperature (°C)']) / 2

    # Add (missing but irrelevant) cloud cover data:
    filtered_df["Cloud cover"] = 0

    # Get rid of negative radiation values
    filtered_df.loc[filtered_df['Shortwave (solar) radiation (W/m2)'] < 0, 'Shortwave (solar) radiation (W/m2)'] = 0

    ###################################################################################################################
    # Write wind data file

    # Convert the time column to minutes relative to the reference date
    filtered_df = convert_to_minutes_relative(filtered_df, "Timestamp", reference_time)

    col_to_wind = ["time_diff_minutes", *wind_columns]
    wind_df = filtered_df.assign(**{col: filtered_df[col].apply(lambda x: f'{x:.7e}') for col in col_to_wind})
    # Replace string "nan" with actual np.nan
    wind_df.replace('nan', np.nan, inplace=True)
    wind_df_clean = wind_df.dropna()

    # Select specified columns
    wind_df_clean = wind_df_clean[col_to_wind]

    ###################################################################################################################
    # Repeat with met data
    col_to_met = ["time_diff_minutes", *met_columns]
    met_df = filtered_df.assign(**{col: filtered_df[col].apply(lambda x: f'{x:.7e}') for col in col_to_met})
    # Select specified columns
    met_df.replace('nan', np.nan, inplace=True)
    met_df_clean = met_df.dropna()
    met_df_clean = met_df_clean[col_to_met]

    ##################################################################################################################
    # Repeat with rainfall data

    # Convert rainfall units from mm/hr to mm/day per Delft3D FM documentation
    filtered_df['Hourly precipitation (mm/hr)'] = 24 * filtered_df['Hourly precipitation (mm/hr)']

    col_to_rain = ["time_diff_minutes", *rain_columns]
    rain_df = filtered_df.assign(**{col: filtered_df[col].apply(lambda x: f'{x:.7e}') for col in col_to_rain})
    # Select specified columns
    rain_df.replace('nan', np.nan, inplace=True)
    rain_df_clean = rain_df.dropna()
    rain_df_clean = rain_df_clean[col_to_rain]
    return wind_df_clean, met_df_clean, rain_df_clean


def gen_forcing(all_files):
    st.header("Create environmental forcing functions from Brusdalen weather station data")

    # Read data into a pandas DataFrame
    met_input = "All_time.csv"
    df = pd.read_csv(met_input, header=0, sep=';', decimal=',')
    df['Time'] = pd.to_datetime(df['Time']).apply(lambda x: x.to_pydatetime())
    min_date = df['Time'].min()
    max_date = df['Time'].max()

    df_profile = Dashboard.upload_hourly_csv_page()
    error_conditions = {
        "Timestamp": (df_profile['Timestamp'] < pd.to_datetime('2000-01-01')) | (
                df_profile['Timestamp'] > pd.to_datetime('2099-12-31')),
        "Temperature (Celsius)": (df_profile['Temperature (Celsius)'] < 1) | (df_profile['Temperature (Celsius)'] > 25),
        "Conductivity (microSiemens/centimeter)": (df_profile['Conductivity (microSiemens/centimeter)'] < 0) |
                                                  (df_profile['Conductivity (microSiemens/centimeter)'] > 45),
        "Specific Conductivity (microSiemens/centimeter)": (
                df_profile['Specific Conductivity (microSiemens/centimeter)'] < 1),
        "Salinity (parts per thousand, ppt)": (df_profile['Salinity (parts per thousand, ppt)'] < 0),
        "pH": (df_profile['pH'] < 2) | (df_profile['pH'] > 12),
        "Dissolved Oxygen (% saturation)": (df_profile['Dissolved Oxygen (% saturation)'] < 10) | (
                df_profile['Dissolved Oxygen (% saturation)'] > 120),
        "Turbidity (NTU)": (df_profile['Turbidity (NTU)'] < 0),
        "Turbidity (FNU)": (df_profile['Turbidity (FNU)'] < 0),
        "fDOM (RFU)": (df_profile['fDOM (RFU)'] < 0) | (df_profile['fDOM (RFU)'] > 100),
        "fDOM (parts per billion QSU)": (df_profile['fDOM (parts per billion QSU)'] < 0) | (
                df_profile['fDOM (parts per billion QSU)'] > 300),
        "Latitude": (df_profile['Latitude'] < -90) | (df_profile['Latitude'] > 90),
        "Longitude": (df_profile['Longitude'] < -180) | (df_profile['Longitude'] > 180)
    }

    # Replace values meeting the error conditions with np.nan using boolean indexing
    for col, condition in error_conditions.items():
        df_profile.loc[condition, col] = np.nan

    df_profile.replace('nan', np.nan, inplace=True)
    df_profile = df_profile.dropna(subset=['Temperature (Celsius)'])

    rc1, rc2, rc3, rc4 = st.columns(4, gap="small")
    with rc1:
        start_date = st.date_input("Select the simulation start date", value=min_date, min_value=min_date,
                                   max_value=max_date, help=None, format="YYYY/MM/DD")
        end_date = st.date_input("Select the simulation end date", value=max_date, min_value=min_date,
                                 max_value=max_date, help=None, format="YYYY/MM/DD")
        reference_date = st.date_input("Select a reference date for the simulation", value=min_date, key=None,
                                       help='Within Delft3D and in output files, all absolute times will '
                                            'be expressed as minutes relative to this selection.', format="YYYY/MM/DD")
        salinity = st.number_input("Salinity:", value=0.02)
        factor = st.number_input("Adjustment Factor:", value=1)

    with rc2:
        start_time = st.time_input("Start time", datetime.time(0, 0))
        end_time = st.time_input("End time", datetime.time(0, 0))
        reference_time = st.time_input("Reference time", datetime.time(0, 0))
        randomization = st.number_input("Randomization Factor:", value=0)
    start = datetime.datetime.combine(start_date, start_time)
    end = datetime.datetime.combine(end_date, end_time)
    if start >= end:
        st.markdown(":red[The start time must be before the end time. Please revise the input.]")
    else:
        reference = datetime.datetime.combine(reference_date, reference_time)
        dfs_met = filter_met(start, end, reference, df)

        # Generate a time series dataframe for each inlet
        nominal_discharge = {"Arsetelva.tim": 0.215,
                             "Brusdalen.tim": 0.044,
                             "S1.tim": 0.028,
                             "S2.tim": 0.021,
                             "S3.tim": 0.021,
                             "S4.tim": 0.023,
                             "Slettebakk.tim": 0.084,
                             "Vasstrandelva.tim": 0.273,
                             "VasstrandliaPump.tim": -0.637}


        dfs_profile = []
        for key, value in nominal_discharge.items():
            dfs_profile.append(filter_profiler(start_date=start, end_date=end, reference_time=reference, df=df_profile, discharge=value, salinity=salinity,
                                               scalefactor=factor, randomize_type=randomization))
        print("# Number of discharges defined:", len(dfs_profile))

        # Set the default directory path to the current directory
        default_directory_path = ''

        # Names for the output files
        met_filenames = ["windxy.tim", "FlowFM_meteo.tim", "rainfall.tim"]
        # inlet_filenames = [f for f in all_files if f.endswith('.tim') and f not in met_filenames]
        inlet_filenames = ["Arsetelva.tim", "Brusdalen.tim", "S1.tim", "S2.tim", "S3.tim", "S4.tim", "Slettebakk.tim",
                           "Vasstrandelva.tim", "VasstrandliaPump.tim"]

        # Ask the user for a directory path with the default as the current directory
        c1, c2 = st.columns(2, gap="small")
        with c1:
            directory_path = st.text_input('Enter a directory path in which to save the forcing files (current '
                                           'directory by default)', value=default_directory_path)
        if st.button("Write weather data to Delft3D input files (.tim)"):
            # Write filtered wind, heat and rainfall data to new CSV files
            for j, frame in enumerate(dfs_met):
                if directory_path:
                    frame.to_csv(f"{directory_path}/{met_filenames[j]}", index=False, header=False, sep=' ')
                else:
                    frame.to_csv(met_filenames[j], index=False, header=False, sep=' ')
            if directory_path:
                st.write(f"Data from selected range exported as {met_filenames} in {directory_path}")
            else:
                st.write(f"Data from selected range exported as {met_filenames} in the working directory")

        print("# of output files defined:", len(inlet_filenames))
        if st.button("Write inlet flow data to Delft3D input files (.tim)"):
            # Write filtered stream data to new CSV files
            for j, frame in enumerate(dfs_profile):
                if directory_path:
                    frame.to_csv(f"{directory_path}/{inlet_filenames[j]}", index=False, header=False, sep=' ')
                else:
                    frame.to_csv(inlet_filenames[j], index=False, header=False, sep=' ')
            if directory_path:
                st.write(f"Data from selected range exported as {inlet_filenames} in {directory_path}")
            else:
                st.write(f"Data from selected range exported as {inlet_filenames} in the working directory")


def post_cor(all_files, directory_path):
    st.header("Compare model outputs against real-world data for calibration")
    filtered_files = [f for f in all_files if f.endswith('his.nc')] + ["Upload your own"]
    hc1, hc2 = st.columns(2, gap="small")
    with hc1:
        selected_file = st.selectbox(label="Select which model output to display", options=filtered_files)
        if directory_path is not None and selected_file != "Upload your own":
            selected_file = os.path.join(directory_path, selected_file)
    # Open hisfile with xarray and print netcdf structure
    ds_his_o = xr.open_mfdataset(selected_file, preprocess=dfmt.preprocess_hisnc)
    ds_his = Dashboard.rename_ds(ds_his_o)

    compatibility = {"Salinity (ppt)": "Salinity (parts per thousand, ppt)",
                     "Temperature (◦C)": "Temperature (Celsius)"}
    errorplots = ["Hourly (depth = 2.95 m)", "Depth profiles (12-hour sample rate)"]
    c1, c2 = st.columns(2, gap='small')
    with c1:
        feature = st.selectbox("Select a variable to compare", compatibility.keys(),index=1)
        column_name = compatibility.get(feature)  # 'feature' name in reference dataset
        location = st.selectbox("Select observation points to plot against the profiler data",
                                ds_his.coords['stations'].values, index=3)
        errorplot = st.radio("Select a sensor dataset for comparison", errorplots, horizontal=True)
    Dashboard.display_error(ds_his=ds_his, feature=feature, column_name=column_name, errorplot=errorplot,
                            errorplots=errorplots, location=location, offline=False)


def spatial_unc(files):
    """
    Calculate 'uncertainty' as the variance (st.dev) of a selected parameter across the map
    :param files: list of map files on which to computer variance. Must share the same grid, time span and data
    variables
    :return:
    """
    ds_list = []
    for i, file in enumerate(files):
        uds_map = dfmt.open_partitioned_dataset(file)
        ds_list.append(Dashboard.rename_ds(uds_map))

    # Create lists of data variables based on their dimensionality
    includes_coordinate = ["mesh2d_nFaces", "time"]
    excludes_coordinates = ["mesh2d_nEdges", "mesh2d_nNodes", "mesh2d_nMax_face_nodes"]
    mesh2d_nFaces_list = []

    uds_map1 = ds_list[0]
    ds_results = uds_map1.copy()
    for name, var in ds_results.data_vars.items():
        if (all(coord in var.dims for coord in includes_coordinate) and all(coord not in var.dims for coord in
                                                                            excludes_coordinates)):
            mesh2d_nFaces_list.append(name)

    c1, c2 = st.columns(2, gap='small')
    with c1:
        variable = st.selectbox("Choose variable to display", mesh2d_nFaces_list)
        timeindices = ['Time-averaged', 'Final']
        timeindex = st.selectbox("Choose between time-averaged uncertainty or uncertainty at the final timestep",
                                 options=timeindices)

    # ds_results = uds_map.assign(differential=uds_map[variable] - uds_map[variable])

    # Step 1: Combine datasets along a new dimension 'dataset'
    combined = xu.concat(ds_list, dim='dataset')

    # Step 2: Calculate the standard deviation along the 'dataset' dimension
    uncertainty = combined[variable].std(dim='dataset')
    depth_avg_Uncert = uncertainty.mean(dim='mesh2d_nLayers')*1E10
    dt_avg_Uncert = depth_avg_Uncert.mean(dim='time')*1E10

    ds_results['Uncertainty'] = uncertainty
    ds_results['Depth-averaged uncertainty'] = depth_avg_Uncert
    ds_results['Depth- and time-averaged uncertainty'] = dt_avg_Uncert

    fig_diff, ax = plt.subplots(figsize=(10,3), dpi=600)
    crs = 'EPSG:4326'
    if timeindex == timeindices[0]:
        pf = ds_results['Depth- and time-averaged uncertainty'].isel(missing_dims='ignore').ugrid.plot(cmap='jet',
                                                                                                       add_colorbar=False)
        st.markdown(f"### Spatial distribution of uncertainty in depth- and time-averaged {variable}")
    elif timeindex == timeindices[1]:
        pf = ds_results['Depth-averaged uncertainty'].isel(time=-1, missing_dims='ignore').ugrid.plot(cmap='jet',
                                                                                                      add_colorbar=False)
        st.markdown(f"### Spatial distribution of uncertainty in depth-averaged {variable}")
    colorbar = plt.colorbar(pf, orientation="vertical", fraction=0.007, pad=0.005)

    # # Get extents of map from attributes, for setting plot limits
    # try:
    #     xmin_abs = uds_map.attrs['geospatial_lon_min']
    # except:
    #     xmin_abs = 6.387478790667275
    #
    # try:
    #     xmax_abs = uds_map.attrs['geospatial_lon_max']
    # except:
    #     xmax_abs = 6.56781074840919
    #
    # try:
    #     ymin_abs = uds_map.attrs['geospatial_lat_min']
    # except:
    #     ymin_abs = 62.46442857883768
    #
    # try:
    #     ymax_abs = uds_map.attrs['geospatial_lat_max']
    # except:
    #     ymax_abs = 62.48487637586751

    # scaler = 1.0
    # aspect = 0.02
    # xavg = (xmax_abs + xmin_abs) / 2
    # yavg = (ymax_abs + ymin_abs) / 2
    # x_int = (xmax_abs - xmin_abs) / 2
    # y_int = (ymax_abs - ymin_abs) / 2
    # xmin = xavg - x_int * (1 + scaler * aspect)
    # xmax = xavg + x_int * (1 + scaler * aspect)
    # ymin = yavg - y_int * (1 + scaler)
    # ymax = yavg + y_int * (1 + scaler)

    xmin = 6.384
    xmax = 6.574
    ymin = 62.461
    ymax = 62.488

    # Set colorbar label
    colorbar.set_label('Sensitivity')
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    ax.set_title("")
    ctx.add_basemap(ax=ax, zoom=15, source=ctx.providers.OpenTopoMap, crs=crs, attribution=False)

    st.pyplot(fig_diff)

    if st.button("Save figure 1"):
        data = Dashboard.download_matplotlib_figure(fig_diff, dpi=600)  # Higher DPI for better resolution
        st.download_button(
            label="Click here to download",
            data=data,
            file_name="Sensitivity.png",
            mime="image/png",
        )

    c1, c2 = st.columns(2, gap='small')
    with c1:
        ranked_points = st.number_input("Choose how many coordinates to display", min_value=0,
                                 max_value=ds_results.sizes['mesh2d_nFaces'], value=10)

    # Plot the selected values on the map
    if timeindex == timeindices[0]:
        df = ds_results[['Depth- and time-averaged uncertainty']].to_dataframe().reset_index()
        df.rename(columns={"mesh2d_face_y": "Latitude", "mesh2d_face_x": "Longitude"}, inplace=True)
        df_sorted = df.sort_values(by='Depth- and time-averaged uncertainty', ascending=False)
        df_sorted.drop('mesh2d_nFaces', axis=1, inplace=True)
        df_sorted.reset_index(drop=True, inplace=True)
        ranked_df = df_sorted[df.index <= (ranked_points - 1)]
        fig = px.scatter_mapbox(ranked_df, lat="Latitude", lon="Longitude", hover_name=ranked_df.index+1,
                                hover_data=['Depth- and time-averaged uncertainty'],
                                color='Depth- and time-averaged uncertainty',
                                color_continuous_scale='Viridis', zoom=3, height=300)
    elif timeindex == timeindices[1]:
        df = ds_results[['Depth-averaged uncertainty']].isel(time=-1).to_dataframe().reset_index()
        df.rename(columns={"mesh2d_face_y": "Latitude", "mesh2d_face_x": "Longitude"}, inplace=True)
        df_sorted = df.sort_values(by='Depth-averaged uncertainty', ascending=False)
        df_sorted.drop('mesh2d_nFaces', axis=1, inplace=True)
        df_sorted.reset_index(drop=True, inplace=True)
        ranked_df = df_sorted[df.index <= (ranked_points - 1)]
        fig = px.scatter_mapbox(ranked_df, lat="Latitude", lon="Longitude", hover_name=ranked_df.index+1,
                                hover_data=['Depth-averaged uncertainty'], color='Depth-averaged uncertainty',
                                color_continuous_scale='Viridis', zoom=3, height=300)
    center_lat = (ranked_df['Latitude'].max() + ranked_df['Latitude'].min()) / 2
    center_lon = (ranked_df['Longitude'].max() + ranked_df['Longitude'].min()) / 2
    fig.update_layout(mapbox_style="open-street-map", mapbox_zoom=12, mapbox_center_lat=center_lat,
                      mapbox_center_lon=center_lon, margin={"r": 0, "t": 0, "l": 0, "b": 0}, width=1500,
                      height=500)
    st.plotly_chart(fig, use_container_width=True)

    # Export the values for use in mission planning
    if st.button("Export ranking of points by uncertainty to CSV"):
        df_sorted.to_csv('Sampling_Priority.csv', index=True)
        st.write(f"{ranked_points} points with greatest uncertainty written to file \'Sampling_Priority.csv\'")


def error_analys(full_files):
    """
    Calculate error statistics for all his files; report which file has the lowest error; use regression to
    estimate the input values which would minimize error.
    :param full_files: list of his files on which to computer variance. Must share the same layers, time span and data
    variables
    :return:
    """
    compatibility = {"Salinity (ppt)": "Salinity (parts per thousand, ppt)",
                     "Temperature (◦C)": "Temperature (Celsius)"}
    errorplots = ["Hourly (depth = 2.95 m)", "Depth profiles (12-hour sample rate)"]
    c1, c2 = st.columns(2, gap='small')
    with c1:
        feature = st.selectbox("Select a variable to compare", compatibility.keys())
        column_name = compatibility.get(feature)  # 'feature' name in reference dataset
    statvalues1 = {'Statistic': ['Correlation',
                                 "Sum of Squares Error", "Mean Absolute Error", "Mean Squared Error",
                                 "Root Mean Squared Error", 'Mean Percent Error']}
    compar_stats = pd.DataFrame(statvalues1)

    statvalues2 = {'Statistic': ['Mean', 'Standard Deviation']}
    ind_stats = pd.DataFrame(statvalues2)
    for i, file in enumerate(full_files):
        ds_his_o = xr.open_mfdataset(file, preprocess=dfmt.preprocess_hisnc)
        ds_his = Dashboard.rename_ds(ds_his_o)
        individual, compar = Dashboard.display_error(ds_his, feature, column_name, errorplot=errorplots[1],
                                            errorplots=errorplots, offline=True)
        compar.index = compar_stats.index
        individual.index = ind_stats.index

        # Append the 'data' column to the existing DataFrame as a new column
        compar_stats[f'Model {i+1}'] = compar['Comparison']
        if i == 0:
            ind_stats[f'Reference Data'] = individual['Reference Data']
        ind_stats[f'Model {i+1}'] = individual['Model']
    c1, c2, c3, c4 = st.columns(4, gap='small')
    with c1:
        st.markdown("##### Per-Series Statistics")
        st.dataframe(ind_stats, hide_index=True)
    with c2:
        st.markdown("##### Comparative Statistics")
        st.dataframe(compar_stats, hide_index=True)

    ##################################################################################################################
    # Add a grouped bar chart compare error values visually

    compar_stats.set_index('Statistic', inplace=True)
    compar_stats = compar_stats.transpose()
    compar_stats.index.names = ['Model']
    compar_stats.reset_index(inplace=True)

    columns_to_normalize = ["Sum of Squares Error", "Mean Absolute Error", "Mean Squared Error",
                                 "Root Mean Squared Error"]

    # Normalize by dividing by the mean
    for column in columns_to_normalize:
        compar_stats[column] = (compar_stats[column] - compar_stats[column].mean())

    hc1, hc2 = st.columns(2, gap="small")
    with hc1:
        plotstats = st.multiselect("Select which comparative statistics to plot",
                                   compar_stats.columns[~(compar_stats.columns == 'Model')])
    compar_stats = compar_stats[[col for col in compar_stats.columns if col in ['Model'] + plotstats]]

    # Melt the DataFrame to long format
    df_melted = compar_stats.melt(id_vars=['Model'], var_name='Difference from mean', value_name='Value')
    # Create a grouped bar chart
    fig = px.bar(df_melted, x='Model', y='Value', color='Difference from mean', barmode='group')

    # Show the plot
    if len(plotstats) > 0:
        st.plotly_chart(fig)


def post_sens(all_files, directory_path):
    st.header("Analyse sensitivity test output")

    output_options = {"Spatial distribution of uncertainty (map file)": 'map.nc',
                      "Parameter tuning (history file)": 'his.nc'}
    d3d_output = st.radio("Select which type of model outputs to display",
                          options=list(output_options.keys()), horizontal=True)

    # Filter files based on extension
    filtered_files = [f for f in all_files if f.endswith(output_options.get(d3d_output))]
    file_options = filtered_files + ["All"]

    hc1, hc2 = st.columns(2, gap="small")
    with hc1:
        selected_files = st.multiselect(label="Select which models to compare", options=file_options,
                                       default="All")
        if "All" in selected_files:
            selected_files = filtered_files
        if directory_path is not None and selected_files != "Upload your own":
            full_files = [os.path.join(directory_path, file) for file in selected_files]
        else:
            full_files = selected_files

    if output_options.get(d3d_output) == 'map.nc':
        # Calculate 'uncertainty' as the variance (st.dev) of a selected parameter
        spatial_unc(full_files)
    elif output_options.get(d3d_output) == 'his.nc':
        # Calculate error statistics for all his files; report which file has the lowest error; use regression to
        # estimate the input values which would minimize error.
        error_analys(full_files)


def pre():
    st.title("Delft3D FM Simulation Configuration")
    functions = ['Adjust simulation run settings', 'Generate forcing data', 'Configure sensitivity test']

    # Ask the user for a directory path with the default as the current directory
    c1, c2 = st.columns(2, gap="small")
    with c1:
        directory_path = st.text_input(
            'Enter the directory containing a Delft3D project\'s input files to find relevant files automatically. '
            'Or, upload individual files as needed in the menus below.')

    if directory_path:
        all_files = os.listdir(directory_path)
    else:
        all_files = os.listdir()

    c1, c2 = st.columns(2, gap="small")
    with c1:
        mode = st.radio("Select activity", functions, horizontal=True)

    if mode == functions[0]:
        gen_MDU(all_files)
    elif mode == functions[1]:
        gen_forcing(all_files)
    elif mode == functions[2]:
        gen_sens()


def post():
    st.title("Delft3D FM Post-Processing")
    functions = ['Display model outputs', 'Compare model to reference data', 'Analyze sensitivity results']

    # Ask the user for a directory path with the default as the current directory
    c1, c2 = st.columns(2, gap="small")
    with c1:
        directory_path = st.text_input(
            'Enter the directory containing Delft3D output files to find relevant files automatically. '
            'Or, upload individual files as needed below.')

    if directory_path:
        all_files = os.listdir(directory_path)
    else:
        all_files = os.listdir()

    c1, c2 = st.columns(2, gap="small")
    with c1:
        mode = st.radio("Select activity", functions, horizontal=True)

    if mode == functions[0]:
        Dashboard.current(all_files, directory_path)
    elif mode == functions[1]:
        post_cor(all_files, directory_path)
    elif mode == functions[2]:
        post_sens(all_files, directory_path)



def main():
    st.set_page_config("Brusdalsvatnet Digital Twin Utility", layout="wide")
    st.sidebar.title("Choose Mode")
    pages = ["Pre-process", "Post-process"]
    selected_page = st.sidebar.radio("", pages)

    if selected_page == pages[0]:
        pre()
    elif selected_page == pages[1]:
        post()


if __name__ == "__main__":
    main()
